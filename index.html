<!DOCTYPE html>
<html lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-71156606-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-71156606-1');
  </script>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>[QIAI] COVID-19 Lung Lesion Segmentation</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <link rel="stylesheet" type="text/css" href="assets/styles/all.css" />
  <script type="text/javascript">document.documentElement.className = 'js';</script>
  <link rel="icon" type="image/svg+xml" href="assets/images/favicon.svg">
  <link rel="mask-icon" href="assets/images/mask-icon.svg" color="#000000">
  <link rel="icon" href="content/images/size/w256h256/2020/09/icon-1.png" type="image/png" />
    <!-- <link rel="canonical" href="https://openai.com/blog/formal-math/" /> -->
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <!-- <meta property="og:site_name" content="OpenAI" /> -->
    <meta property="og:type" content="article" />
    <meta property="og:title" content="COVID-19 Lung Lesion Segmentation" />
    <!-- <meta property="og:description" content="We built a neural theorem prover for Lean [https://leanprover.github.io/] that
learned to solve a variety of challenging high-school olympiad problems,
including problems from the AMC12
[https://www.maa.org/math-competitions/amc-1012] and AIME
[https://www.maa.org/math-competitions/invitational-competitions] competitions,
as well as two problems adapted from" /> -->

<link rel="stylesheet" href="/path/to/styles/default.min.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.8.0/Chart.bundle.min.js"></script>
<script src="./Plugin.Errorbars.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.1.0/papaparse.min.js"></script>

<script src="/path/to/highlight.min.js"></script>
<script>hljs.highlightAll();</script>

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "OpenAI",
        "url": "https://openai.com/",
        "logo": {
            "@type": "ImageObject",
            "url": "content/images/2022/05/openai-avatar.png",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "Stanislas Polu",
        "url": "https://openai.com/blog/authors/stanislas/",
        "sameAs": []
    },
    "headline": "Solving (Some) Formal Math Olympiad Problems",
    "url": "https://openai.com/blog/formal-math/",
    "datePublished": "2022-02-02T18:01:48.000Z",
    "dateModified": "2022-02-07T17:00:14.000Z",
    "keywords": "Research",
    "description": "We built a neural theorem prover for Lean [https://leanprover.github.io/] that\nlearned to solve a variety of challenging high-school olympiad problems,\nincluding problems from the AMC12\n[https://www.maa.org/math-competitions/amc-1012] and AIME\n[https://www.maa.org/math-competitions/invitational-competitions] competitions,\nas well as two problems adapted from the IMO [https://www.imo-official.org/].[1] \nThe prover uses a language model to find proofs of formal statements. Each time\nwe find a new ",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://openai.com/"
    }
}
    </script>
    <meta name="generator" content="Ghost 5.2" />
    <link rel="alternate" type="application/rss+xml" title="OpenAI" href="https://openai.com/blog/rss/" />
    <script defer src="_tryghost/portal__2.2.0/umd/portal.min.js" data-ghost="https://openai.com/" data-key="23f7e2ddba37b787ce1ea90e77" data-api="https://openaidev.ghost.io/ghost/api/content/" ></script><style id="gh-members-styles">.gh-post-upgrade-cta-content,
.gh-post-upgrade-cta {
    display: flex;
    flex-direction: column;
    align-items: center;
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
    text-align: center;
    width: 100%;
    color: #ffffff;
    font-size: 16px;
}
.gh-post-upgrade-cta-content {
    border-radius: 8px;
    padding: 40px 4vw;
}
.gh-post-upgrade-cta h2 {
    color: #ffffff;
    font-size: 28px;
    letter-spacing: -0.2px;
    margin: 0;
    padding: 0;
}
.gh-post-upgrade-cta p {
    margin: 20px 0 0;
    padding: 0;
}
.gh-post-upgrade-cta small {
    font-size: 16px;
    letter-spacing: -0.2px;
}
.gh-post-upgrade-cta a {
    color: #ffffff;
    cursor: pointer;
    font-weight: 500;
    box-shadow: none;
    text-decoration: underline;
}
.gh-post-upgrade-cta a:hover {
    color: #ffffff;
    opacity: 0.8;
    box-shadow: none;
    text-decoration: underline;
}
.gh-post-upgrade-cta a.gh-btn {
    display: block;
    background: #ffffff;
    text-decoration: none;
    margin: 28px 0 0;
    padding: 8px 18px;
    border-radius: 4px;
    font-size: 16px;
    font-weight: 600;
}
.gh-post-upgrade-cta a.gh-btn:hover {
    opacity: 0.92;
}</style>
    <script defer src="public/cards.min.js"></script><style>:root {--ghost-accent-color: #15171A;}</style>
    <link rel="stylesheet" type="text/css" href="public/cards.min.css">
</head>
<body>
<article class="post" id="post-formal-math">
  <header
  class="post-header"
  >
  <!-- <nav class="nav container" data-url="/blog/formal-math/">
  <div class="nav-row row align-items-center">
    <div class="d-none d-sm-block col-sm nav-symbol-wrap">
      <a href="index.html" class="nav-symbol"><svg id="openai-symbol" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 51 51"><path d="M47.21,20.92a12.65,12.65,0,0,0-1.09-10.38A12.78,12.78,0,0,0,32.36,4.41,12.82,12.82,0,0,0,10.64,9a12.65,12.65,0,0,0-8.45,6.13,12.78,12.78,0,0,0,1.57,15A12.64,12.64,0,0,0,4.84,40.51a12.79,12.79,0,0,0,13.77,6.13,12.65,12.65,0,0,0,9.53,4.25A12.8,12.8,0,0,0,40.34,42a12.66,12.66,0,0,0,8.45-6.13A12.8,12.8,0,0,0,47.21,20.92ZM28.14,47.57a9.46,9.46,0,0,1-6.08-2.2l.3-.17,10.1-5.83a1.68,1.68,0,0,0,.83-1.44V23.69l4.27,2.47a.15.15,0,0,1,.08.11v11.8A9.52,9.52,0,0,1,28.14,47.57ZM7.72,38.85a9.45,9.45,0,0,1-1.13-6.37l.3.18L17,38.49a1.63,1.63,0,0,0,1.65,0L31,31.37V36.3a.17.17,0,0,1-.07.13L20.7,42.33A9.51,9.51,0,0,1,7.72,38.85Zm-2.66-22a9.48,9.48,0,0,1,5-4.17v12a1.62,1.62,0,0,0,.82,1.43L23.17,33.2,18.9,35.67a.16.16,0,0,1-.15,0L8.54,29.78A9.52,9.52,0,0,1,5.06,16.8ZM40.14,25,27.81,17.84l4.26-2.46a.16.16,0,0,1,.15,0l10.21,5.9A9.5,9.5,0,0,1,41,38.41v-12A1.67,1.67,0,0,0,40.14,25Zm4.25-6.39-.3-.18L34,12.55a1.64,1.64,0,0,0-1.66,0L20,19.67V14.74a.14.14,0,0,1,.06-.13L30.27,8.72a9.51,9.51,0,0,1,14.12,9.85ZM17.67,27.35,13.4,24.89a.17.17,0,0,1-.08-.12V13a9.51,9.51,0,0,1,15.59-7.3l-.3.17-10.1,5.83a1.68,1.68,0,0,0-.83,1.44Zm2.32-5,5.5-3.17L31,22.35v6.34l-5.49,3.17L20,28.69Z"/></svg></a>
    </div>
    <div class="col col-sm-auto">
      <ul class="d-flex flex-row align-items-center justify-content-between small-caps">
        <div class="d-sm-none nav-symbol-wrap">
          <a href="index.html" class="nav-symbol"><svg id="openai-symbol" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 51 51"><path d="M47.21,20.92a12.65,12.65,0,0,0-1.09-10.38A12.78,12.78,0,0,0,32.36,4.41,12.82,12.82,0,0,0,10.64,9a12.65,12.65,0,0,0-8.45,6.13,12.78,12.78,0,0,0,1.57,15A12.64,12.64,0,0,0,4.84,40.51a12.79,12.79,0,0,0,13.77,6.13,12.65,12.65,0,0,0,9.53,4.25A12.8,12.8,0,0,0,40.34,42a12.66,12.66,0,0,0,8.45-6.13A12.8,12.8,0,0,0,47.21,20.92ZM28.14,47.57a9.46,9.46,0,0,1-6.08-2.2l.3-.17,10.1-5.83a1.68,1.68,0,0,0,.83-1.44V23.69l4.27,2.47a.15.15,0,0,1,.08.11v11.8A9.52,9.52,0,0,1,28.14,47.57ZM7.72,38.85a9.45,9.45,0,0,1-1.13-6.37l.3.18L17,38.49a1.63,1.63,0,0,0,1.65,0L31,31.37V36.3a.17.17,0,0,1-.07.13L20.7,42.33A9.51,9.51,0,0,1,7.72,38.85Zm-2.66-22a9.48,9.48,0,0,1,5-4.17v12a1.62,1.62,0,0,0,.82,1.43L23.17,33.2,18.9,35.67a.16.16,0,0,1-.15,0L8.54,29.78A9.52,9.52,0,0,1,5.06,16.8ZM40.14,25,27.81,17.84l4.26-2.46a.16.16,0,0,1,.15,0l10.21,5.9A9.5,9.5,0,0,1,41,38.41v-12A1.67,1.67,0,0,0,40.14,25Zm4.25-6.39-.3-.18L34,12.55a1.64,1.64,0,0,0-1.66,0L20,19.67V14.74a.14.14,0,0,1,.06-.13L30.27,8.72a9.51,9.51,0,0,1,14.12,9.85ZM17.67,27.35,13.4,24.89a.17.17,0,0,1-.08-.12V13a9.51,9.51,0,0,1,15.59-7.3l-.3.17-10.1,5.83a1.68,1.68,0,0,0-.83,1.44Zm2.32-5,5.5-3.17L31,22.35v6.34l-5.49,3.17L20,28.69Z"/></svg></a>
        </div>
          <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link " href="index.html" data-slug="api">API</a></li>
          <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link " href="index.html" data-slug="research">Research</a></li>
          <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link active-parent" href="index.html" data-slug="blog">Blog</a></li>
          <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link " href="index.html" data-slug="about">About</a></li>
      </ul>
    </div>
  </div> -->

  <style>
    .underline-effect {
  background-image: linear-gradient(120deg, #c400ad65 0%, #0092af91 100%);
  background-repeat: no-repeat;
  background-size: 100% 0.25em;
  background-position: 0 80%;
}
.underline-effect-2 {
  background-image: linear-gradient(120deg, #00a3c484 0%, #0092af86 100%);
  background-repeat: no-repeat;
  background-size: 100% 0.25em;
  background-position: 0 80%;
}
.underline-effect-3 {
  background-image: linear-gradient(120deg, #00c46984 0%, #00af6f86 100%);
  background-repeat: no-repeat;
  background-size: 100% 0.25em;
  background-position: 0 80%;
}
mark {
  background-color: rgba(255, 225, 0, 0.689);
  /* border-radius: 10px; */
  color: black;
}
  </style>
</nav>
          <div class="container mt-5">
    <div class="row">
        <div class="col-12 col-md-9 col-lg-8 col-xl-12 text-xl-center">
          <div class="max-width-xxwide mx-xl-auto">
            <h1 class="
   balance-text
   mb-0.75 mb-xl-2.25
   
  "><span class="underline-effect">COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN on Chest X-rays Automatically Computed from Volumetric CTs</span></h1>          </div>
        </div>
    </div>
      <div class="row">
        <div class="col-12 col-md-9 col-lg-8 col-xl-6 order-xl-1">
            <div class="post-excerpt content no-col js-excerpt-container js-widow">
  </div>
        </div>
        <div class="col-12 col-md-3 col-lg-4 col-xl-3 order-xl-0">
            <div class="post-header-date small-copy color-fg-50 mb-1.5">
    <time datetime="2022-02-02"><span style="color: black">May 17, 2021</span></time>
    <div class="reading-time" style="position: relative; top: 10px">Presented at the <span style="color: black">2021 Annual Meeting of the Society for Imaging Informatics in Medicine (<strong><span class="underline-effect-2"><a href="https://siim.org/page/siim2021" target=_blank>SIIM21</a></span></strong>)</span></div>
    <div class="reading-time" style="position: relative; top: 20px"><span style="color: black">Cite as: </span>
      <pre style="width: 83%; position: relative; top:-20px"><code ><span style="color: black">V. Ramesh, B. Rister, and D. L. Rubin.
“COVID-19 Lung Lesion Segmentation 
Using a Sparsely Supervised Mask R-CNN
on Chest X-rays Automatically Computed 
from Volumetric CTs.” arXiv:2105.08147
[eess.IV], May 2021.
</code></pre>
</div></div></div></div>
 </div>
  </div>
</header>
  <section class="container">
  <div class="row">
    <section class="content">
      <!--kg-card-begin: markdown--><div class="js-excerpt" >
<p> Chest X-rays of COVID-19 patients are frequently obtained 
  to determine the extent of lung disease and are a valuable source of data for creating 
  AI models. Most work to date assessing disease severity on chest 
  imaging has focused on segmenting CT images; however, given that 
  CTs are performed much less frequently than chest X-rays for COVID-19 patients, automated 
  lung lesion segmentation on chest X-rays is clinically valuable. To <mark>accelerate 
  severity detection</mark> and <mark>augment the amount of publicly available chest X-ray training data 
  for supervised DL models</mark>, we propose an automated pipeline for segmentation of COVID-19 lung lesions 
  on chest X-rays comprised of a <mark style="background-color: rgba(0, 225, 255, 0.689);">Mask R-CNN</mark> trained on <span style="font-variant: small-caps"><mark style="background-color: rgba(255, 0, 234, 0.689);">CheXMix</mark></span>, our newly released dataset containing a <mark style="background-color: rgba(0, 225, 255, 0.689);">mixture</mark> of open-source chest 
  X-rays and <mark style="background-color: rgba(0, 225, 255, 0.689);">coronal X-ray projections computed from annotated volumetric CTs</mark>. 
  
  <!-- In this paper, we propose an
  interpretable <mark>“Contextual Generator”</mark> architecture for question answering (QA),
  built as an extension of the recently published <mark>“Generator”</mark> algorithm for sentence generation<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>, 
  that produces grammatically valid answers to queries structured as lists of seed words. 
  We demonstrate the potential for this architecture to
  perform automated, closed-domain QA by detailing results on queries from
  <a href="https://singularitynet.io/" target=_blank>SingularityNET</a>’s “small world” <mark style="background-color: rgba(0, 225, 255, 0.689);">POC-English corpus</mark> and from the <mark style="background-color: rgba(0, 225, 255, 0.689);"><strong>S</strong>tanford
  <strong>Qu</strong>estion <strong>A</strong>nswering <strong>D</strong>ataset</mark> (<a href="https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/" target=_blank>SQuAD 2.0</a>). Overall, our work may bring a greater degree of
  general conversational intelligence to proto-AGI NLP pipelines. -->
  
</div>


<section class="btns" ><a href="https://arxiv.org/abs/2105.08147" class="btn btn-dark btn-padded icon-paper">Read Paper</a> 
  <a href="https://github.com/rvignav/CT2Xray" class="btn btn-dark btn-padded icon-code">View Code</a>
  <a href="https://youtu.be/GGDTEq_liLE" class="btn btn-dark btn-padded icon-play">Watch Presentation</a>
  <a href="https://github.com/rvignav/CheXMix" class="btn btn-dark btn-padded icon-layers">View Dataset</a></section>
<hr >
<p >
  We develop an automated pipeline for COVID-19 lung lesion segmentation on chest X-rays. Due to the
lack of publicly available annotated chest X-ray data, we implement a pixel-based algorithm (a method
operating at the pixel level) that generates coronal X-ray projections from annotated volumetric CTs to
augment the training dataset. A Mask R-CNN framework is then trained on this mixed dataset. Our
model achieves superior accuracy with only limited supervised training.
</p>
 

<h2 ><span class="underline-effect-2">CT &rarr; CXR Conversion</span></h1>

  <p >
    We implement a pixel-based re-projection method, modeled as a sub-problem
of ray tracing<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>, to compute chest X-rays as coronal projections of volumes of axial CT
slices:</p>

<div class="proof color-fg font-sans-serif rounded p-0.75 p-md-1" data-id="1">
  <div class="informal-statement medium-small-copy"><div class="small-caps mb-0.5">Equation 1: CT to X-ray Conversion</div>
  <!-- <div class="note">Adapted from AMC12B 2020 Problem 6</div> -->
  We perform the following summation: <br><br><span style="font-size: x-large"><center>$\Theta(x,z) = \sum_{i=1}^{Y} \Phi(x,i,z) \quad \forall (x,z) \in [1, X] × [1, Z] ,$<br><br></center></span>
  where $\Phi$ is an $X \times Y \times Z$ 3D array denoting the volumetric CT and $\Theta$ is an $X \times Z$ matrix denoting the computed X-ray.
  </div><!-- end informal statement  -->
  </div><!-- end proof -->

  <p>The algorithm is implemented in Python as follows:</p>

  <div  class="proof color-fg font-sans-serif rounded p-0.75 p-md-1" data-id="0">
    <div class="informal-statement medium-small-copy"><div class="small-caps mb-0.5">Algorithm 1: create_CXR()</div>
    <!-- <div class="note">Adapted from AMC12 2000 Problem 5</div> -->

    <pre><code class="language-python">from PIL import Image
import numpy as np

def create_CXR(images): # images is a list of axial CT slices
  for z in range(len(images)): 
    img = Image.open(images[z]).convert('L')  # convert image to 8-bit grayscale
    HEIGHT, WIDTH = img.size
    data = list(img.getdata()) # convert image data to a list of integers
    # convert that to 2D list (list of lists of integers)
    pixels = [data[offset:offset+WIDTH] for offset in range(0, WIDTH*HEIGHT, WIDTH)]

    xray = np.zeros((len(images), Image.open(images[0]).convert('L').size[0]))

    # Loop from left to right on the CT slice
    for x in range(WIDTH):
      # Sum y values in the current x column
      sum = 0
      for y in range(HEIGHT):
        sum += pixels[y][x]
        # Assign sum to the point (x, z) on the coronal image - p[z][x] in the pixel array, 
        # since z represents height (rows) and x represents length (columns)
        xray[len(images) - 1 - z][x] = sum
  
  return xray</code></pre>
    </div><!-- end formal -->
    </div>

    <div class="full bg-fg-3 pt-3 pb-3" id="sample"><div class="container">
  
      <div class="row mb-0.125"><div class="content">
      <!-- <button class="button-unstyled fade js-refresh-sample" style="visibility: visible;" onclick="refreshSample()">
      <span class="small-copy"><span class="icon font-xxsmall mr-1/12 position-relative" style="top:0.15em;left:-1px">refresh</span>View another passage</span>
      </button> -->
      </div></div>
      <!-- passage -->
    
    
    <center><img src="Figure1.png" style="width:65%">
    
    <hr class="my-1" style="max-width:840px">
    <div class="caption my-0" style="max-width:840px">
    <p>
      Our algorithm uses <span style="color:black">(a)</span> a volumetric CT and <span style="color:black">(b)</span> a volume of
      mask slices to compute <span style="color:black">(c)</span> an annotated coronal CXR.
    
    
    </p>
    </div></center>
      
      <!-- summary -->
      
      <!-- critiques -->
      
      </div></div>
    





<h2 ><span class="underline-effect-2"><span style="font-variant: small-caps">CheXMix</span>: a <strong>che</strong>st <strong>X</strong>-ray dataset containing a <strong>mix</strong>ture of patient X-rays and coronal CT projections with COVID-19 lung lesion annotations</span></h1>
    

<p >We present <span style="font-variant: small-caps">CheXMix</span>, the first publicly available, open-source chest X-ray dataset containing over 100 images
  (assembled from <a href="https://coronacases.org/" target="_blank">a</a> <a href="https://radiopaedia.org/" target=_blank>variety</a> <a href="https://zenodo.org/record/3757476#.Yq1KpuzML8E" target=_blank>of</a> <a href="https://arxiv.org/abs/2005.06465" target=_blank>public</a> <a href="https://covid-segmentation.grand-challenge.org/Data/" target="_blank">sources</a>) with COVID-19 lung lesion annotations
  produced by our Mask R-CNN model:</p>





  <div id="samples-content" class="full mb-1.5 position-relative overflow-hidden" style="background-color:#f6f6f4; ">
    <div class="js-carousel carousel pt-4 pb-2.5">
    <div class="js-carousel-item carousel-item" id="xrays-only" style="height: 510px"><div class="container">
    <div class="row mb-1.5">

      <iframe src="https://www.kaggle.com/datasets/rvignav/xrays-only" style="border-radius: 10px; width:100%; height:500px"></iframe>

      </div>
    
  
  
    </div></div>
    
    <div class="js-carousel-item carousel-item" id="mixed" style="height: 510px"><div class="container">
      <div class="row mb-1.5">
  
        <iframe src="https://www.kaggle.com/datasets/rvignav/xrays-only" style="border-radius: 10px; width:100%; height:500px"></iframe>
  
        </div>
      
    
    
      </div></div>

  
      <div class="js-carousel-item carousel-item" id="test" style="height: 510px"><div class="container">
        <div class="row mb-1.5">
    
          <iframe src="https://www.kaggle.com/datasets/rvignav/xrays-only" style="border-radius: 10px; width:100%; height:500px"></iframe>
    
          </div>
        
      
      
        </div></div>
    
      
  
    
    
    </div><!-- end .carousel -->
    </div><!-- end #samples-content -->





    <h2 ><span class="underline-effect-2">Lung Lesion Segmentation</span></h1>

      <p>We employ a naive implementation of the Mask R-CNN framework for the task of instance segmentation.
        In a Mask R-CNN architecture, training samples are fed into a ResNet-101 backbone network, convolved, and passed to the Region Proposal Network (RPN) to generate a set of proposed regions
        possibly containing lung lesions. Anchors corresponding with each region of interest are then passed
        through a series of feature maps to generate masks outlining COVID-19 lung lesions on the input chest
        X-ray. Object classes and bounding boxes are computed via a series of fully connected layers. The task
        of COVID-19 lung lesion segmentation is posed as a problem of binary classification between the image
        background and lung lesions. The final output is a predicted mask corresponding with the input chest
        X-ray, which can then be overlaid on the input image for clinical use.</p>
  
  
<div class="full bg-fg-3 pt-3 pb-3" id="sample"><div class="container">
  
  <div class="row mb-0.125"><div class="content">
  <!-- <button class="button-unstyled fade js-refresh-sample" style="visibility: visible;" onclick="refreshSample()">
  <span class="small-copy"><span class="icon font-xxsmall mr-1/12 position-relative" style="top:0.15em;left:-1px">refresh</span>View another passage</span>
  </button> -->
  </div></div>
  <!-- passage -->


<center><img src="diagram-1.png" style="width:80%">

<hr class="my-1" style="max-width:840px">
<div class="caption my-0" style="max-width:840px">
<p>
  <span style="color:black">Mask R-CNN network architecture.</span> Training images are fed into a backbone network, which then passes a network representation of the training samples to the Region Proposal Network
  (RPN). The RPN scans the top-bottom pathway of the backbone network and proposes regions which
  may contain objects of interest on the feature map. RPN anchors are then fed into a series of feature
  maps, allowing for the parallel execution of two operations: 1) the creation of masks; and 2) the generation
  of object classes and bounding boxes using a series of fully connected layers.</p>
</div></center>
  
  <!-- summary -->
  
  <!-- critiques -->
  
  </div></div>

  <h3 ><span class="underline-effect-3">Environmental Setup</span><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></h1>


<p >
  Our models were trained on a single GPU (Tesla P4 GPU provided by Google Colab, 16 GB memory). The code is implemented using TensorFlow v1, but is compatible with TensorFlow v2 and can be ported to the <a href="https://www.tensorflow.org/versions" target="_blank">most recent version of TensorFlow</a> if desired. To install all required dependencies, run the following:
</p>

<pre><code class="language-bash">pip install tensorflow==1.15.2 keras==2.1.0 Pillow scikit-image opencv-python numpy glob2 regex os-sys argparse matplotlib</code></pre>

<p>Afterwards, set up the Mask R-CNN model:</p>

<pre><code class="language-bash">git clone --quiet https://github.com/rvignav/Mask_RCNN.git
cd ~/Mask_RCNN
pip install -q PyDrive
python setup.py install
cp ~/Mask_RCNN/samples/balloon/balloon.py ./lesion.py
sed -i -- 's/balloon/lesion/g' lesion.py
sed -i -- 's/Balloon/Lesion/g' lesion.py</code></pre>

<h3 ><span class="underline-effect-3">Training</span></h1>

  <p>The following commands can be used to train the Mask R-CNN model:  </p>

  <pre><code class="language-bash"># Train a new model starting from pre-trained ImageNet weights
python lesion.py train --dataset='/path/to/data/' --weights=imagenet
    
# Train a new model starting from pre-trained COCO weights
python lesion.py train --dataset='/path/to/data/' --weights=coco
    
# Continue training a model that you had trained earlier
python lesion.py train --dataset='/path/to/data/' --weights=/path/to/weights/
    
# Continue training the last model you trained. This will find
# the last trained weights in the model directory.
python lesion.py train --dataset='/path/to/data/' --weights=last</code></pre>

<p>To train with data augmentation<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>, run:</p>

<pre><code class="language-bash">python lesion.py train --dataset='/path/to/data/' --weights=imagenet/coco/last --aug='y'</code></pre>

<p>The CT to X-ray re-projection algorithm can be executed in isolation as follows:</p>

<pre><code class="language-bash">python ct2xray.py &langle;path/to/CT/volume&rangle; &langle;path/to/mask/volume&rangle;</code></pre>

  <h3 ><span class="underline-effect-3">Pre-trained Models</span></h1>

    <table>
      <tr>
        <th>Training dataset</th>
        <th>Train/test split</th>
        <th>Data augmentation (y/n)</th>
        <th>Checkpoint</th>
      </tr>
      <tr>
        <td>X-rays only</td>
        <td>60/40</td>
        <td>y</td>
        <td><span class="underline-effect-2"><a style="text-decoration: none" href="https://drive.google.com/file/d/1Db0NhVCIBOJJTfDHjtmgm3I10-KsUpg-/view?usp=sharing" target=_blank>Download</a></span></td>
      </tr>
      <tr>
        <td>Mixed</td>
        <td>60/40</td>
        <td>y</td>
        <td><span class="underline-effect-2"><a style="text-decoration: none" href="https://drive.google.com/file/d/1nizSK5_RQXsaQ-omKtKL3dwaLL2xJnfC/view?usp=sharing" target=_blank>Download</a></span></td>
      </tr>
      <tr>
        <td>X-rays only</td>
        <td>80/20</td>
        <td>y</td>
        <td><span class="underline-effect-2"><a style="text-decoration: none" href="https://drive.google.com/file/d/15TBvC-UUYZ4OB_ExNCewHNrZFXdDCPZR/view?usp=sharing" target=_blank>Download</a></span></td>
      </tr>
      <tr>
        <td>Mixed</td>
        <td>80/20</td>
        <td>y</td>
        <td><span class="underline-effect-2"><a style="text-decoration: none" href="https://drive.google.com/file/d/1cO2ck9sJm79tmW-FvawO_ogIL_4yLFpU/view?usp=sharing" target=_blank>Download</a></span></td>
      </tr>
      <tr>
        <td>X-rays only</td>
        <td>80/20</td>
        <td>n</td>
        <td><span class="underline-effect-2"><a style="text-decoration: none" href="https://drive.google.com/file/d/1fNQndbTef8bu-OPJZHUio4CtTgQMKKxr/view?usp=sharing" target=_blank>Download</a></span></td>
      </tr>
      <tr>
        <td>Mixed</td>
        <td>80/20</td>
        <td>n</td>
        <td><span class="underline-effect-2"><a style="text-decoration: none" href="https://drive.google.com/file/d/11Bs9XbJNKPXaVzKWydvR6r6j9cOFf5ig/view?usp=sharing" target=_blank>Download</a></span></td>
      </tr>
    </table>

<h2><span class="underline-effect-2">Results</span></h1>
  

  <canvas id="container" style="width:100%; height:400px"></canvas>


<br>
<p>Our model's results far exceed the few existing published baselines. For instance, Tang, Sun, and Li’s U-Net segmentation model<sup>*</sup> (the only published COVID-19 lung lesion segmentation
  framework with publicly available model schematics), when implemented and trained on Datasets 1 and
  2, achieved <strong>I</strong>ntersection <strong>o</strong>ver <strong>U</strong>nion (IoU) scores of 0.38 ± 0.03 and 0.49 ± 0.03, respectively, both of which are significantly lower
  than our model’s corresponding IoU scores of 0.81 ± 0.03 and 0.79 ± 0.03. Since we trained and tested
  our model and the baseline model on the same datasets, our Mask R-CNN likely outperformed Tang,
  Sun, and Li’s U-Net segmentation architecture due to its structure as a series of recurring feature maps
  rather than contracting and expansive paths, the presence of the RPN, and its greater complexity in
  the form of a ResNet-101 backbone rather than a ResNet-18 backbone.</p>
  
  <p>Furthermore, the fact that model achieved similar results after training on both Datasets 1 and 2 indicates that we can replace more than 83% of chest X-ray training images with X-ray projections
    generated from CTs while maintaining model accuracy.</p>

<p>Representative results are shown in the figures below.</p>

<div class="full bg-fg-3 pt-3 pb-3" id="sample"><div class="container">
  
  <div class="row mb-0.125"><div class="content">
  <!-- <button class="button-unstyled fade js-refresh-sample" style="visibility: visible;" onclick="refreshSample()">
  <span class="small-copy"><span class="icon font-xxsmall mr-1/12 position-relative" style="top:0.15em;left:-1px">refresh</span>View another passage</span>
  </button> -->
  </div></div>
  <!-- passage -->


<center><img src="Screen Shot 2022-06-18 at 10.26.26 PM.png" style="width:150%">

<hr class="my-1" style="max-width:840px">
<div class="caption my-0" style="max-width:840px">
<p>
  <span style="color:black">Left: Ground truth and predicted masks.</span> Sample of five chest X-rays from the test dataset (X-rays are the same within individual columns). The top
  row (white) displays the ground truth masks, the middle row (green) contains the masks predicted by
  the model after training on Dataset 1, and the bottom row (blue) contains the masks predicted by the
  model after training on Dataset 2. <br><br><span style="color:black">Right: Overlaid segmentations.</span> The top row displays
  the original X-rays, the middle row contains the predicted segmentations overlaid on the X-rays after
  training on Dataset 1, and the bottom row contains the predicted segmentations overlaid on the X-rays
  after training on Dataset 2.


</p>
</div></center>
  
  <!-- summary -->
  
  <!-- critiques -->
  
  </div></div>




  <h2><span class="underline-effect-2">What next?</span></h2>

  <p>A limitation of our study is that we used small amounts of publicly available data; however, our results
    still suggest that improved accuracy can be obtained by augmenting chest X-ray data with large numbers
    of frontal projections of public CT volumes. Training and testing our model on larger datasets could
    improve future results.</p>
    <br><br>

<style>
:root {
  --proof-green: #00A67D; /* green */
  --proof-medium-blue: #0082D0; /* medium-blue */
  --proof-red: #F22C3D; /* red */
  --proof-golden: #EA9100; /* golden */
}
.proof {
  background-color: #0E0E1A;
  --fg: 247,247,247;
  margin-top: calc(var(--v) * 1.5);
  margin-bottom: calc(var(--v) * 1.5);
}
.proof .note {
  color: rgba(var(--fg), 0.5);
  font-style: italic;
}
.proof .theorem {
  color: var(--proof-red);
  font-weight: bold;
}
.proof .keyword {
  color: var(--proof-green);
}
.proof .comment {
  color: rgba(var(--fg), 0.5);
  font-style: italic;
}
.proof .model {
  color: var(--proof-golden);
}
.proof .highlight {
  font-weight: bold;
  background-color: rgba(var(--fg), 0.05);
}
.proof + .proof {
  margin-top: calc(var(--v) * -0.5);
}
.proof pre {
  margin-top: 0;
  margin-bottom: 0;
}
.informal p {
  margin-bottom: 0;
}
</style>
<style>
  #toxicity-chart svg {
    width: 100%;
    height: 0.333333em;
    margin-bottom: calc(var(--v) * 0.375);
  }
  #samples-note > * {
    display: none;
    visibility: hidden;
  }
  #samples-note[data-active="0"] [data-id="0"],
  #samples-note[data-active="1"] [data-id="1"],
  #samples-note[data-active="2"] [data-id="2"],
  #samples-note[data-active="3"] [data-id="3"],
  #samples-note[data-active="4"] [data-id="4"],
  #samples-note[data-active="5"] [data-id="5"] {
    display: block;
    visibility: visible;
  }
  #samples .highlight {
    background-color: rgba(0,183,255,0.25);
    display: inline;
  }
  .carousel {
    opacity: 0;
    transition: opacity 250ms ease-in-out;
  }
  .carousel.flickity-enabled {
    opacity: 1;
  }
  .carousel-item {
    width: 100%;
    display: none;
  }
  .carousel-item.is-selected {
    z-index: 1; /* so works with iframe fade */
  }
  .carousel-item:first-of-type,
  .flickity-enabled .carousel-item {
    display: block;
  }
  /* flickity */
  .flickity-enabled {
    position: relative;
  }
  .flickity-enabled:focus { outline: none; }
  .flickity-viewport {
    /*overflow: hidden;*/
    position: relative;
    height: 100%;
  }
  .flickity-slider {
    position: absolute;
    width: 100%;
    height: 100%;
  }
  /* flickity button */
  .flickity-button {
    position: absolute;
    background-color: rgba(255,255,255,0.7);
    -webkit-backdrop-filter: blur(1.5rem);
    backdrop-filter: blur(1.5rem);
    border: none;
    opacity: 0;
    transition: opacity 250ms ease-in-out;
    cursor: default;
  }
  .flickity-enabled:hover .flickity-button:not([disabled]) {
    opacity: 0.7;
  }
  .flickity-enabled:hover .flickity-button:not([disabled]):hover {
    opacity: 1;
    cursor: pointer;
  }
  .flickity-button:focus {
    outline: none;
  }
  .flickity-button-icon {
    fill: rgba(0,0,0,1);
  }
  /* flickity previous/next buttons */
  .device-type-mobile .flickity-prev-next-button {
    display: none;
  }
  .flickity-prev-next-button {
    top: calc(50% + var(--v) * 1);
    width: 2.5rem;
    height: 2.5rem;
    border-radius: 50%;
    transform: translateY(calc(-50% - 1.5rem));
  }
  .flickity-prev-next-button.previous { left: 0; }
  .flickity-prev-next-button.next { right: 0; }
  .flickity-rtl .flickity-prev-next-button.previous {
    left: auto;
    right: 0;
  }
  .flickity-rtl .flickity-prev-next-button.next {
    right: auto;
    left: 0;
  }
  .flickity-prev-next-button .flickity-button-icon {
    position: absolute;
    left: 0;
    top: 0;
    width: 100%;
    height: 100%;
  }
  /* flickity page dots */
  .flickity-page-dots {
    margin-bottom: 0 !important;
    display: -webkit-box;
    display: -ms-flexbox;
    display: flex;
    -webkit-box-pack: center;
    -ms-flex-pack: center;
    justify-content: center;
    width: 100%;
    position: absolute;
    top: calc(var(--v) * 1);
  }
  .flickity-rtl .flickity-page-dots { direction: rtl; }
  .flickity-page-dots .dot {
    cursor: pointer;
    display: block;
    cursor: pointer;
    padding: 1rem 0.15rem;
    width: 2.5rem;
  }
  .flickity-page-dots .dot:before {
    content: none;
  }
  .flickity-page-dots .dot:after {
    content: "";
    display: block;
    width: 100%;
    height: 2px;
    border-radius: 1px;
    opacity: 0.2;
    background-color: rgba(var(--fg), 1);
  }
  .flickity-page-dots .dot:hover:after {
    opacity: 0.8;
  }
  .flickity-page-dots .dot.is-selected:after {
    opacity: 1;
  }
  </style>
<script src="flickity_2.2.1/dist/flickity.pkgd.min.js"></script>
<script src="flickity-hash_1/hash.js"></script>
<script>
  // Flickity carousel
  var initCarousel = function (mainClass, cellClass) {
    var el = document.querySelector(mainClass);
    if (!el) return;
    var flickity = new Flickity(el, {
      cellSelector: cellClass,
      wrapAround: true,
      draggable: false,
      pageDots: true,
      hash: true,
      adaptiveHeight: true,
      selectedAttraction: 0.15,
      friction: 0.72,
      arrowShape: 'M55.18,32.24l2.56,2.54L42.65,50,57.74,65.22l-2.56,2.54L37.59,50Z',
      on: {
        ready: function () {
          initCarouselStyles();
        }
      },
    });
    flickity.on('change', function () {
      setActiveNote(flickity.selectedIndex);
    });
  };
  var setActiveNote = function (i) {
    var note = document.getElementById('samples-note');
    if (!note) return;
    note.setAttribute('data-active', i);
  };
  var initCarouselStyles = function () {
    var buttons = document.querySelectorAll('.flickity-prev-next-button');
    if (buttons.length) {
      buttons.forEach(function (button) {
        button.classList.add('mx-col');
      });
    }
    var pageDots = document.querySelector('.flickity-page-dots');
    if (pageDots) {
      pageDots.classList.add('list-unstyled');
    }
  };
  // call everything
  document.addEventListener('DOMContentLoaded', function () {
    initCarousel('.js-carousel', '.js-carousel-item');
  });
  </script>

<script>
  var xValues = ["Dataset 1 (X-rays only)","Dataset 2 (Mixed)"];
  
  new Chart("myChart", {
    type: "line",
    data: {
      labels: xValues,
      datasets: [{ 
        // label: 'BLEU&uarr;',
        data: [0.8056, 0.7937],
        borderColor: "blue",
        fill: false
      }, { 
        // label: 'Word2Vec Cosine Similarity&uarr;',
        data: [0.3824, 0.4870],
        borderColor: "green",
        fill: false
      }]
    },
    options: {
      legend: {display: false}
    }
  });
  </script>
<script src="./script.js"></script>
<script>
  // get and randomize JSON samples, https://stackoverflow.com/a/35294675
  // var filePath = "https://cdn.openai.com/critiques/draft-20220605a/";
  // var filePath = "https://gist.githubusercontent.com/justinjaywang/b14e9d05c8203a158dac4c5a26cf8017/raw/d2be835b7d4b4514c3cb1545e85f7fe9915d7499/";
  var filePath = "https://gist.githubusercontent.com/justinjaywang/e167409e9b21edde2378df90fba2a52a/raw/00ba4d779b892f16b19fa1882cce94261b480a43/";
  var samples = {
    // file: "critique-samples-test.json",
    file: "critiques-samples.json",
    pairs: [
      {key: 'passage', selector: '[data-fill="passage"]'},
      {key: 'question', selector: '[data-fill="question"]'},
      {key: 'answer_human', selector: '[data-fill="answer_human"]'},
      {key: 'answer_human_misleading', selector: '[data-fill="answer_human_misleading"]'},
      {key: 'answer_model', selector: '[data-fill="answer_model"]'},
    ],
    critiques: [
     {key: 'human_critiques_unassisted', selector: '[data-fill="human_critiques_unassisted"]'},
     {key: 'human_critiques_assisted_model', selector: '[data-fill="human_critiques_assisted_model"]'},
     {key: 'human_critiques_assisted', selector: '[data-fill="human_critiques_assisted"]'},
     {key: 'human_misleading_critiques_unassisted', selector: '[data-fill="human_misleading_critiques_unassisted"]'},
     {key: 'human_misleading_critiques_assisted_model', selector: '[data-fill="human_misleading_critiques_assisted_model"]'},
     {key: 'human_misleading_critiques_assisted', selector: '[data-fill="human_misleading_critiques_assisted"]'},
     {key: 'model_critiques_unassisted', selector: '[data-fill="model_critiques_unassisted"]'},
     {key: 'model_critiques_assisted_model', selector: '[data-fill="model_critiques_assisted_model"]'},
     {key: 'model_critiques_assisted', selector: '[data-fill="model_critiques_assisted"]'},
    ],
  };
  var openRequest = function () {
    var request = new XMLHttpRequest();
    request.open('GET', filePath + samples['file'], true);
    request.onload = function() {
      if (request.status >= 200 && request.status < 400) {
        // Success!
        var data = JSON.parse(request.responseText);
        samples.l = data.length;
        samples.data = data;
        showRefresh();
      } else {
        // We reached our target server, but it returned an error
        console.log("error after reaching server with ", file)
      }
    };
    request.onerror = function() {
      // There was a connection error of some sort
      console.log("request error with ", file)
    };
    request.send();
  };
  // open request
  openRequest();
  var showRefresh = function () {
    sampleEl = document.getElementById('sample');
    sampleEl.querySelector('.js-refresh-sample').style.visibility = 'visible';
  };
  var refreshSample = function () {
    var i = rand(samples.l);
    var sample = samples.data[i - 1];
    // scroll to top of passage
    var p = document.getElementById('passage');
    p.scrollTop = 0;
    // replace text in simple pairs
    samples.pairs.forEach(function (pair) {
      var sampleStr = sample[pair.key];
      var formattedSampleStr = smarten(sampleStr.trim().replace(/\n/g, '<br />'));
      // replace DOM
      document.querySelector(pair.selector).innerHTML = formattedSampleStr;
    });
    // replace text in critiques
    samples.critiques.forEach(function (critique) {
     var critiquesEl = document.querySelector(critique.selector);
     critiquesEl.innerHTML = ''; // clear out
      var critArr = sample[critique.key]; // array of critique objects
      if (!critArr.length) {
        // no critiques, append "none" message
        var c = document.createElement('div');
        c.classList.add('color-fg-50', 'font-italic');
        c.innerHTML = '(none)';
        critiquesEl.appendChild(c); // append to DOM
      }
      critArr.forEach(function (critObj) {
        // append each critique to parent div
        var critStr = critObj.critique;
        var formattedcritStr = smarten(critStr.trim().replace(/\n/g, '<br />'));
        var isUnhelpful = !!critObj.is_unhelpful;
        var c = document.createElement('div');
        if (isUnhelpful) c.classList.add('unhelpful');
        c.innerHTML = formattedcritStr;
        critiquesEl.appendChild(c); // append to DOM
      });
    });
  };
  var rand = function (l) {
    return Math.floor((Math.random() * l) + 1);
  };
  // https://gist.github.com/drdrang/705071
  var smarten = function (a) {
    a = a.replace(/(^|[-\u2014\s(\["])'/g, "$1\u2018");       // opening singles
    a = a.replace(/'/g, "\u2019");                            // closing singles & apostrophes
    a = a.replace(/(^|[-\u2014/\[(\u2018\s])"/g, "$1\u201c"); // opening doubles
    a = a.replace(/"/g, "\u201d");                            // closing doubles
    a = a.replace(/--/g, "\u2014");                           // em-dashes
    return a
  };
  // toggle function
  var toggle = function (whichIds, otherIds) {
    for (var i = 0; i < whichIds.length; i++) {
      var whichId = whichIds[i];
      var whichEls = document.querySelectorAll('[data-id="' + whichId + '"]');
      if (!whichEls.length) return;
      whichEls.forEach(function (e) {
        e.style.display = 'block';
      });
    }
    for (var i = 0; i < otherIds.length; i++) {
      var otherId = otherIds[i];
      var otherEls = document.querySelectorAll('[data-id="' + otherId + '"]');
      if (!otherEls.length) return;
      otherEls.forEach(function (e) {
        e.style.display = 'none';
      });
    }
  };
  // togglers
  var initToggler = function () {
    var togglers = document.querySelectorAll('.js-toggler');
    if (!togglers.length) return;
    for (var i = 0; i < togglers.length; i++) {
      var toggler = togglers[i];
      toggler.addEventListener('click', function (e) {
        removeActiveTogglers(this.parentElement.querySelectorAll('.js-toggler'));
        addActiveToggler(this);
      });
    }
  };
  var addActiveToggler = function (el) {
    el.classList.add('active');
  };
  var removeActiveTogglers = function (els) {
    els.forEach(function (el) {
      el.classList.remove('active');
    });
  };
  // init
  document.addEventListener('DOMContentLoaded', function () {
    initToggler();
  });
  </script>

<footer class="post-footer js-post-footer" style="margin-top: 0px">

<div data-order="-2"><hr><div class="row" id="footnotes">
<div class="col">Footnotes</div>
<div class="col">
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item">
  <p>the act of using the path that light takes through pixels to create images <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>

<li id="fn2" class="footnote-item"><p>
 <span style="color:black">Default Setting:</span> We trained for $30$ epochs,
each with $200$ training steps and $50$ validation steps, at batch size $2$. We used ResNet-101 as the base
encoder network with backbone strides of $4$, $8$, $16$, $32$, and $64$ and a top-down pyramid size of $256$. For the
losses, we used RPN class loss, RPN bounding box loss, Mask R-CNN class loss, Mask R-CNN bounding
box loss, and Mask R-CNN mask loss. We used a gradient clip norm of $5.0$, an image shape of $1024 \times
1024 \times 3$, a learning momentum of $0.9$, a learning rate of $0.001$, a weight decay of $0.0001$, and a mask
shape of $28 \times 28$. For the RPN specifications, we used anchor ratios of $0.5$, $1$, and $2$; anchor scales of $32$, $64$, $128$, $256$, and $512$; 
an anchor stride of $1$; a ROI positive ratio of $0.33$; bounding box standard
deviations of $0.1$ and $0.2$; and $200$ training ROIs per image. The maximum number of ground truth
instances is $15$. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>

<li id="fn3" class="footnote-item"><p> The following
  augmentation techniques were applied to the training samples with specified probabilities in parentheses: horizontal flip ($0.5$), $0$-$10$% random crop ($1.0$), 
  small Gaussian blur with randomly chosen $\sigma \in [0,0.5]$ ($0.5$), contrast normalization, per-channel pixel multiplication with $\delta \in [0.8,1.2]$ ($0.2$), affine transformation ($1.0$),
  scale, rotate, &amp; shear. <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<!-- <li id="fn4" class="footnote-item"><p>
    It is seen that the Generator falls victim to the issue of <span style="color: black">grammatical
    ambiguity</span>, which refers to situations in which <span style="color: black">the same word may have different roles
    in a sentence</span> (i.e. a noun can be either a subject or an object) or <span style="color: black">may represent different
    parts of speech</span> (such as the word “saw” in its verb and noun forms). <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>
  a metric derived from <span style="color:black">Zipf's Law</span> <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li> -->

</ol>
</section>
<!--kg-card-end: markdown-->
    </section>
  </div>
</section>
  <footer class="post-footer post-footer--authors container js-post-footer-authors">
  <div data-order="0">
    <hr>
    <div class="row" id="authors">
      <div class="col">Authors</div>
      <div class="col js-post-footer-authors-list ">
        <span class="post-author"><a class="fade" href="https://vignavramesh.me">Vignav Ramesh</a></span><span class="post-author"><a class="fade" href="https://orcid.org/0000-0002-4490-0444">Blaine Rister</a></span><span class="post-author"><a class="fade" href="http://rubinlab.stanford.edu/">Daniel L. Rubin</a></span></span>
      </div>
    </div>
  </div>
</footer>

</article>

  <script type="text/javascript" src="assets/scripts/main.js"></script>
  <br>
<center><font size="2"></font></center></body>
</html>
